{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Second Assignment\n",
    "\n",
    "For each question provide an answer in the designated area. Try to be brief and precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "A binary classifier produced the confusion matrix shown below. Calculate the precision in this case. What is the size of the corpus?\n",
    "\n",
    "![confusion-matrix 1](https://tsourakis.net/tec640/confusion-matrix1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9904559915164369, 10115)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given values from the confusion matrix\n",
    "true_positives = 5604\n",
    "false_positives = 54\n",
    "false_negatives = 1890\n",
    "true_negatives = 2567\n",
    "\n",
    "# Precision calculation\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "# Size of the corpus\n",
    "corpus_size = true_positives + false_positives + false_negatives + true_negatives\n",
    "\n",
    "precision, corpus_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Precision**: The precision of the binary classifier with the given confusion matrix is approximately 99.05%.\n",
    "\n",
    "2. **Size of the Corpus**: The size of the corpus is 10,115."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The three models that are shown below (depicted with the red line) try to separate the data with labels __X__ and __O__ . Which one do you think generalizes<sup>*</sup> better (left/middle/right)? Explain your answer.\n",
    "\n",
    "![models](https://tsourakis.net/tec640/models.png)\n",
    "\n",
    "<sup>*</sup>Generalization is the ability of a model to classify correctly unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Left Model**: This model has a linear decision boundary. It seems to be too simplistic and likely underfitting the data. It does not capture the pattern that the \"O\" labeled data points are clustered in the middle and the \"X\" labeled data points are surrounding them. It misclassifies many \"O\" points as \"X.\"\n",
    "\n",
    "2. **Middle Model**: This model has a non-linear decision boundary that wraps around the cluster of \"O\" labeled points. It does a good job separating the two classes without being too complex or too simple. It seems to balance well between bias and variance, suggesting that it is likely to generalize better to new, unseen data.\n",
    "\n",
    "3. **Right Model**: This model has a very complex decision boundary that perfectly separates all the \"X\" and \"O\" labeled points in the training set. This suggests overfitting, where the model is too complex and captures noise in the data alongside the underlying pattern. Such a model is likely to perform poorly on new, unseen data because it's too tailored to the specific quirks of the training data.\n",
    "\n",
    "Based on these observations, the **middle model** is likely the one that generalizes better. It strikes a reasonable balance between underfitting and overfitting, capturing the underlying pattern in the data while still being flexible enough to adapt to new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Based on the images below, which one of the ROC curves produces AUC values greater than 0.5 (left/middle/right)? Explain your answer.\n",
    "\n",
    "![roc 1](https://tsourakis.net/tec640/roc1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Left Curve**: This curve bows significantly towards the top-left corner of the plot, which indicates a high True Positive Rate for most thresholds and a low False Positive Rate for most thresholds. This is characteristic of a model with good performance, and hence this curve will have an AUC greater than *0.5*.\n",
    "\n",
    "- **Middle Curve**: This is a diagonal line from the bottom left to the top right. This represents a random classifier (for example, flipping a coin to predict the class) and would have an AUC of exactly *0.5*.\n",
    "\n",
    "- **Right Curve**: This curve also bows towards the top-left corner, though not as steeply as the left curve, which indicates a good but not perfect performance. This model also has an AUC greater than *0.5*.\n",
    "\n",
    "So, both the **left and right** ROC curves produce AUC values greater than *0.5*, while the **middle** curve represents an AUC of exactly *0.5*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The image below shows a special type for word representation. How is it called and what can we deduce from this image?\n",
    "\n",
    "![representation](https://tsourakis.net/tec640/representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image likely represents a ***\"word embedding\"*** space or a ***\"vector space model,\"*** where words (or in this case, the names of countries and their capital cities) are mapped into vectors in a continuous vector space. \n",
    "From the image, we can deduce:\n",
    "\n",
    "1. **Geographic Proximity**: Capitals are near their respective countries, which reflects their real-world association.\n",
    "\n",
    "2. **Semantic Similarity**: Countries and cities that are close to each other in this space are likely to be contextually related in the data on which the model was trained.\n",
    "\n",
    "This kind of word representation is powerful for many NLP tasks because it enables the algorithm to understand relationships and analogies between different words, significantly improving the quality of tasks like translation, sentiment analysis, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Among the three options below, what is the best option on how to split our data (left/middle/right)? Elaborate on your answer.\n",
    "\n",
    "![split](https://tsourakis.net/tec640/split.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Left Option**: This is the standard approach, where the dataset is split into 70% for training and 30% for testing. This split is commonly used in machine learning to ensure that the model is trained on a majority of the data while still having a substantial amount of unseen data for testing its performance.\n",
    "\n",
    "2. **Middle Option**: Here, the dataset is split with 70% for testing and 30% for training. This is not a typical split because it leaves less data for the model to learn from, which can lead to a poorly trained model that does not perform well.\n",
    "\n",
    "3. **Right Option**: The 'no split' approach uses the entire dataset for both training and testing. This is not advisable because it can lead to overfitting, where the model performs very well on the training data but poorly on any unseen data since it has essentially memorized the dataset.\n",
    "\n",
    "The **left option** is therefore the recommended option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "The following images show the confusion matrices for two classification models. Which model works better and why (left or right)?\n",
    "\n",
    "![confusion-matrix 2](https://tsourakis.net/tec640/confusion-matrix2.png)\n",
    "![confusion-matrix 3](https://tsourakis.net/tec640/confusion-matrix3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.9102643856920685,\n",
       "  array([0.95794099, 0.9375848 , 0.87082262, 1.        , 1.        ,\n",
       "         0.84788895]),\n",
       "  array([0.99673416, 0.9829303 , 0.99926254, 0.56      , 0.65770863,\n",
       "         0.97214854]),\n",
       "  array([0.97695262, 0.95972222, 0.93063187, 0.71794872, 0.79351536,\n",
       "         0.90577695])),\n",
       " (0.998289269051322,\n",
       "  array([1.        , 1.        , 0.99559471, 0.99837662, 1.        ,\n",
       "         0.9973545 ]),\n",
       "  array([1.        , 0.99857752, 1.        , 0.984     , 1.        ,\n",
       "         1.        ]),\n",
       "  array([1.        , 0.99928826, 0.99779249, 0.99113618, 1.        ,\n",
       "         0.9986755 ])))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Confusion matrices for both models\n",
    "confusion_matrix1 = np.array([\n",
    "    [1526, 0, 5, 0, 0, 0],\n",
    "    [2, 691, 3, 0, 0, 7],\n",
    "    [0, 0, 1355, 0, 0, 1],\n",
    "    [9, 1, 146, 350, 0, 119],\n",
    "    [56, 45, 5, 0, 465, 136],\n",
    "    [0, 0, 42, 0, 0, 1466]\n",
    "])\n",
    "\n",
    "confusion_matrix2 = np.array([\n",
    "    [1531, 0, 0, 0, 0, 0],\n",
    "    [0, 702, 0, 1, 0, 0],\n",
    "    [0, 0, 1356, 0, 0, 0],\n",
    "    [0, 0, 6, 615, 0, 4],\n",
    "    [0, 0, 0, 0, 707, 0],\n",
    "    [0, 0, 0, 0, 0, 1508]\n",
    "])\n",
    "\n",
    "# Function to calculate performance metrics\n",
    "def calculate_performance(confusion_matrix):\n",
    "    # True Positives are on the diagonal\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    \n",
    "    # False Positives are the sum of each column, minus the diagonal\n",
    "    FP = np.sum(confusion_matrix, axis=0) - TP\n",
    "    \n",
    "    # False Negatives are the sum of each row, minus the diagonal\n",
    "    FN = np.sum(confusion_matrix, axis=1) - TP\n",
    "    \n",
    "    # True Negatives are the sum of all values, minus the sum of the FP, FN, and TP for each class\n",
    "    TN = np.sum(confusion_matrix) - (FP + FN + TP)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = np.sum(TP) / np.sum(confusion_matrix)\n",
    "    \n",
    "    # Precision and recall for each class\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    \n",
    "    # F1 score for each class\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "# Calculate performance for both models\n",
    "performance1 = calculate_performance(confusion_matrix1)\n",
    "performance2 = calculate_performance(confusion_matrix2)\n",
    "\n",
    "performance1, performance2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First model**:\n",
    "- The accuracy is approximately 91.03%.\n",
    "- Precision ranges from about 84.79% to 100%, varying significantly among classes.\n",
    "- Recall ranges from about 56% to 99.67%, also varying significantly among classes.\n",
    "- The F1 score, which is the harmonic mean of precision and recall, ranges from about 71.79% to 97.70%.\n",
    "\n",
    "**Second model**:\n",
    "- The accuracy is significantly higher at approximately 99.83%.\n",
    "- Precision is very high, with the lowest being about 99.56% and others at 100%.\n",
    "- Recall is also very high, all above 98.4%.\n",
    "- The F1 scores are near perfect, with the lowest at about 99.11% and others at 100%.\n",
    "\n",
    "Based on these metrics, the **second model** outperforms the first model across all measures. It has a higher accuracy, precision, recall, and F1 scores are consistently high for all classes. This suggests that the second model is much better at correctly classifying all classes without significant misclassification. The first model, while still performing reasonably well, shows a particular weakness in class D, with a recall of only 56% and a correspondingly lower F1 score. \n",
    "\n",
    "Therefore, the second model would be the preferred choice for deployment based on this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Based on the confusion matrix below, what can you say about the relation between labels __D__ and __C__?\n",
    "\n",
    "![confusion-matrix 2](https://tsourakis.net/tec640/confusion-matrix2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Misclassification between D and C**: There are 146 instances where class C is incorrectly predicted as class D. This is a relatively high number compared to other off-diagonal numbers, suggesting that the model often confuses class C with class D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "After performing *Exploratory Data Analysis (EDA)* on our data we get the pie chart shown below. It represents the sentiment found in the texts of the corpus.\n",
    "Based on the chart, can you identify any issue, and if yes, how can it be resolved?\n",
    "\n",
    "![pie-chart](https://tsourakis.net/tec640/pie-chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few issues were identified but here are the main two:\n",
    "\n",
    "1. **Overlapping or Ambiguous Categories**: Sentiment categories such as *\"thumbs up\"* and *\"happy/joyful\"* might overlap, as could *\"thumbs down\"* and *\"unhappy.\"* This could make it difficult to distinguish between subtly different sentiments.\n",
    "   - **Resolution**: Merge overlapping categories or redefine them to reduce ambiguity.\n",
    "\n",
    "2. **Granularity of Sentiments**: The sentiment categories range from very specific (e.g., *\"thumbs up/down\"*) to more broad (e.g., *\"empowered\"* or *\"unempowered\"*). This variation in granularity could skew the analysis.\n",
    "   - **Resolution**: Ensure consistent granularity across sentiment categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Suppose that you create two classification models that can predict cancer based on patients' data. Which metric is more appropriate in order to compare the two models? Precision or recall? Explain your answer.\n",
    "\n",
    "__Hint__: Think of the type of errors used by each metric and which error should be avoided for cancer detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting cancer, the choice between precision and recall is literally a life and death call:\n",
    "\n",
    "- **Precision** High precision means that when the model predicts cancer, it is likely to be correct, which minimizes false alarms (false positives).\n",
    "\n",
    "- **Recall** High recall means that the model is good at identifying most of the positive cases, which minimizes the chances of missing a patient with cancer (false negatives).\n",
    "\n",
    "**In cancer prediction, recall is more important than precision**. This is because the cost of a false negative (failing to identify a patient with cancer) can be very high, potentially leading to a lack of treatment and worse health outcomes for the patient. On the other hand, while false positives can lead to unnecessary stress and further testing, the consequences are generally less severe than missing a true positive case. The false positives can also be mitigated by conducting a second test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Using the KNN algorithm what is the label of the sample with the question mark when k=9?\n",
    "\n",
    "![knn](https://tsourakis.net/tec640/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For \\( k=3 \\)**: There are 2 circles (o) and 1 cross (x).\n",
    "- **Expanding to \\( k=6 \\)**: There are 4 circles (o) and 2 crosses (x), including the counts from \\( k=3 \\).\n",
    "- **Expanding further to \\( k=9 \\)**: There are 6 circles (o) and 3 crosses (x), including the counts from \\( k=6 \\).\n",
    "\n",
    "Based on the count for \\( k=9 \\), there are more circles than crosses within the 9 nearest neighbors. Therefore, the label for the sample with the question mark when \\( k=9 \\) would be a circle (o), as the majority label among the 9 closest points isare circles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "Based on the ROC curves below which classification algorithms combination works better? Explain your answer.\n",
    "\n",
    "![roc 2](https://tsourakis.net/tec640/roc2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tandalone *SVM* classifier and *\"SVM + Naive Bayes\"* classifier works better for this particular task, as indicated by the ROC curve and AUC values. There is a slight difference reagrding the  false positives and false negatives, with a small edge for the standalone *SVM* classifier. The ultimate choice of classifier might depend on the specific operational trade-offs between false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "How do we call the type of learning where we use labeled examples during training (see the image below)?\n",
    "\n",
    "![training](https://tsourakis.net/tec640/training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of learning depicted in this image, where labeled examples are used during the training phase, is called *\"supervised learning\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "In the code shown below, how do we call _kernel_, _C_ and _gamma_?\n",
    "\n",
    "``from sklearn import svm``\n",
    "\n",
    "``svm_classifier = svm.SVC(kernel=\"rbf\", C=1.0, gamma=1.0)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters kernel, C, and gamma are hyperparameters of a *Support Vector Machine (SVM)* classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "Based on the figure below, how do we call the transformation of the words in the second column to the words in the third one?\n",
    "\n",
    "![words](https://tsourakis.net/tec640/words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation of words in the second column to the words in the third column is called \"stemming.\" The words \"connect,\" \"connected,\" \"connection,\" \"connections,\" and \"connects\" are all reduced to the stem \"connect.\" to normalize words to their base or root form in order to reduce the complexity of textual data and improve the performance of text processing tasks such as search queries, information retrieval, and text classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15\n",
    "\n",
    "You use a dataset with 10,000 labeled examples to train a classification algorithm. Then, during the inference phase, you use the same examples to calculate the performance of the generated model. The latter gives you 99% accuracy. Can you know launch your model in the production system? Explain your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model cannot be launched into production for the following reasons:\n",
    "1. **Overfitting**: Using all the labeled examples for training can lead to overfitting, where the model performs exceptionally well on the training data but may not generalize well to new data because the model is too tailored to the specific noise and details of the training set.\n",
    "\n",
    "2. **Lack of Validation**: To assess a model's performance effectively, we should have a separate validation set that the model has never seen before. This helps to simulate how the model would perform on new data.\n",
    "\n",
    "3. **Need for a Test Set**: Even beyond a validation set, a separate test set (a third, independent set of data) is typically used to give a final performance estimate before going live in production. This helps to assess the model’s performance in a real-world scenario.\n",
    "\n",
    "4. **Model Biases**: Training and testing on the same data do not expose potential biases in the model. A model that performs well on the training set may simply be reflecting the biases within that data rather than any underlying, generalizable pattern.\n",
    "\n",
    "5. **Real-world Conditions**: The conditions under which a model is trained can be quite different from those it will encounter in a production environment. Without testing the model on different data, you cannot be confident that it will perform well under real-world conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "\n",
    "Consider the vectors shown in the image below _A=(4, 4, 4)_, _B=(1, 7, 5)_ and _C=(-5, 5, 1)_. Which is the method that can tell us which pair of vectors is more similar?\n",
    "\n",
    "![vectors](https://tsourakis.net/tec640/vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cosine similarity to measure the cosine of the angle between two vectors as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8666666666666666, 0.08084520834544429, 0.5659164584181102)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Define the vectors\n",
    "A = (4, 4, 4)\n",
    "B = (1, 7, 5)\n",
    "C = (-5, 5, 1)\n",
    "\n",
    "# Calculate cosine similarity (since scipy's cosine gives the distance, we subtract from 1 to get similarity)\n",
    "similarity_AB = 1 - cosine(A, B)\n",
    "similarity_AC = 1 - cosine(A, C)\n",
    "similarity_BC = 1 - cosine(B, C)\n",
    "\n",
    "similarity_AB, similarity_AC, similarity_BC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity scores between the pairs of vectors are as follows:\n",
    "\n",
    "| Similarity between | Score |\n",
    "|--------------------|-------|\n",
    "| A and B            | 0.867 |\n",
    "| A and C            | 0.081 |\n",
    "| B and C            | 0.566 |\n",
    "\n",
    "\n",
    "These scores indicate that **vectors A and B are the most similar pair, with the highest cosine similarity score**. The pair A and C have the lowest similarity score, indicating they are the least similar. The similarity between B and C falls in between the other two pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17\n",
    "\n",
    "How can we create plots like the one shown in the figure below in Python and how are they called?\n",
    "\n",
    "![nlp](https://tsourakis.net/tec640/nlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is called a *\"Word Cloud\"* or *\"Tag Cloud.\"* It is a visual representation of text data, typically used to depict keyword metadata (tags). Tags are usually single words, and the importance of each tag is shown with font size or color. \n",
    "\n",
    "To create a Word Cloud in Python, we use the wordcloud library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18\n",
    "\n",
    "\n",
    "You are managing a team of data scientists that produced the following table after testing various machine learning algorithms.\n",
    "It is your job to decide which algorithm to use for the problem under study. Which one would you choose and why?\n",
    "\n",
    "![results](https://tsourakis.net/tec640/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering only the metrics in the table and assuming that computational resources and interpretability are not primary concerns, I would choose the Random Forest classifier due to its high accuracy, precision, recall, and F-score. This indicates that it is likely to generalize well to new data. However, if interpretability were a key factor, I might consider the CART algorithm, which also has high metrics but is generally more interpretable than a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19\n",
    "\n",
    "Suppose that your machine learning problem consists of a large set of features with similar characteristics. What is the technique to identify and decrease the number of features for this specific problem? Name one example of this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique used to identify and decrease the number of features in a dataset is called *\"feature selection\"* or *\"dimensionality reduction.\" *This process involves selecting a subset of relevant features (variables, predictors) that contribute most to the prediction variable or output in which we are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20\n",
    "\n",
    "In most machine learning problems, you need to create your own dataset. For example, suppose you want to train a model to identify where to add extra armor to a fighting plane. After each battle, you examine all the aircraft that managed to return back to the base despite being damaged. The red dots in the image below represent the hits you have counted from those aircraft. Try to think where you should add the extra armor by looking at the figure. Can you identify the problem of creating your dataset like this?\n",
    "\n",
    "\n",
    "![plane](https://tsourakis.net/tec640/plane.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\"><b>Answer:<b><p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image and the described scenario is the ***classic example of survivorship bias***. The problem with creating a ***dataset based on the planes that returned with damage is that it only includes a non-random sample of survivors*** — planes that were hit and were still able to return to base. What's missing from the dataset are the planes that did not survive after being hit; those likely suffered damage in critical areas that led to their loss.\n",
    "\n",
    "For a truly effective armor strategy, we would ***need to analyze data from planes that were lost as well as those that returned***, which might require different methods of data collection or historical reconstruction of lost aircraft's damage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
